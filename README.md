# Combining Intermediate Layers for Knowledge Distillation in Neural Machine Translation Models for Japanese -> English


## Requirements
Check `README_CKD_Original.md`

## Data Preparation

## Acknowledgement
This repo is exploration based on original source at [CKD_PyTorch](https://github.com/yimeng0701/CKD_pytorch) which is the original implementation of the paper `Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers`, Yimeng Wu, Peyman Passban, Mehdi Rezagholizadeh, Qun Liu at Proceedings of EMNLP, 2020.